<!-- DO NOT EDIT THIS FILE MANUALLY -->

<!-- Please read https://github.com/the-horizon-dev/docker-fast-whisper/blob/main/.github/CONTRIBUTING.md -->

# the-horizon-dev/docker-fast-whisper

> **Fast, dropâ€‘in Whisper API** â€“ *Fasterâ€‘Whisper served over HTTP, inspired by the official OpenAI Whisper API.*

[Fasterâ€‘Whisper](https://github.com/SYSTRAN/faster-whisper) is a reâ€‘implementation of OpenAIâ€™s Whisper model built on top of **CTranslate2**, delivering dramatically faster inference on CPU and GPU alike. This container turns Fasterâ€‘Whisper into a selfâ€‘hosted service with an HTTP interface that mirrors the OpenAI Whisper API, so you can swap endpoints without rewriting your application code.

---

## ğŸ™Â Credits & lineage

This image is a **fork** of the outstanding work by the **[LinuxServer.io](https://linuxserver.io) team**: [https://github.com/linuxserver/docker-faster-whisper](https://github.com/linuxserver/docker-faster-whisper).
We reused their build pipeline, base image and a good chunk of their documentation.â€¯Huge thanks to the LSIO maintainers for openâ€‘sourcing the original container and CI setup!

---

## ğŸ—ï¸  Supported architectures

Multiâ€‘arch manifests are published so `docker pull ghcr.io/the-horizon-dev/fast-whisper:latest` will grab the correct image for your host:

| Architecture | Tag pattern         |
| ------------ | ------------------- |
| `x86â€‘64`     | `amd64-<version>`   |
| `arm64`      | `arm64v8-<version>` |

> **Note:** We currently do **not** ship an `armhf` build.

---

## ğŸ”–  Version tags

| Tag      | Description                             |
| -------- | --------------------------------------- |
| `latest` | Stable CPUâ€‘only build                   |
| `gpu`    | Same as `latest`, compiled with CUDA 12 |

---

## ğŸš€  Quick start

### DockerÂ Compose (recommended)

```yaml
services:
  fast-whisper:
    image: ghcr.io/the-horizon-dev/fast-whisper:latest
    container_name: fast-whisper
    environment:
      - PUID=1000            # user id to run as
      - PGID=1000            # group id
      - TZ=Etc/UTC
      - WHISPER_MODEL=tiny-int8
      - WHISPER_BEAM=1       # optional
      - WHISPER_LANG=en      # optional
      - PORT=8000            # optional
    volumes:
      - /path/to/whisper/config:/config
    ports:
      - 8000:8000
    restart: unless-stopped
```

### DockerÂ CLI

```bash
docker run -d \
  --name fast-whisper \
  -e PUID=1000 \
  -e PGID=1000 \
  -e TZ=Etc/UTC \
  -e WHISPER_MODEL=tiny-int8 \
  -e WHISPER_BEAM=1 \
  -e WHISPER_LANG=en \
  -e PORT=8000 \
  -v /path/to/whisper/config:/config \
  -p 8000:8000 \
  --restart unless-stopped \
  ghcr.io/the-horizon-dev/fast-whisper:latest
```

If you use the `gpu` tag, add the `--gpus` flag and make sure the [NVIDIAÂ Container Toolkit](https://github.com/NVIDIA/nvidia-container-toolkit) is installed on the host.

---

## ğŸ”Œ  API usage

The container exposes endpoints that mimic the official OpenAI Whisper API:

* `POST /v1/audio/transcriptions` â€“ transcribe a local file or remote URL
* `POST /v1/audio/translations` â€“ translate nonâ€‘English speech to English

Send exactly the same JSON body you would send to `api.openai.com` â€“ only the URL and your auth headers change.

See the [reference docs](https://github.com/SYSTRAN/faster-whisper#usage) for modelâ€‘specific query parameters.

---

## âš™ï¸  Parameters & environment variables

| Variable        | Description                                       | Default     |
| --------------- | ------------------------------------------------- | ----------- |
| `WHISPER_MODEL` | Model name (including `-int8` quantised variants) | `tiny-int8` |
| `WHISPER_BEAM`  | Number of decoding beams                          | `1`         |
| `WHISPER_LANG`  | ISOÂ 639â€‘1 language code                           | `en`        |
| `PORT`          | Internal HTTP port                                | `8000`      |
| `PUID` / `PGID` | UID/GID the process runs as                       | `1000`      |
| `UMASK`         | Override default umask                            | `022`       |

All variables can also be provided through Docker *secrets* by prefixing the name with `FILE__`.

---

## ğŸ”„  Updating the container

```bash
docker compose pull fast-whisper
docker compose up -d fast-whisper
```

Remember to prune dangling images afterwards:

```bash
docker image prune
```

---

## ğŸ› ï¸  Building locally

```bash
git clone https://github.com/the-horizon-dev/docker-fast-whisper.git
cd docker-fast-whisper
docker build -t ghcr.io/the-horizon-dev/fast-whisper:latest .
```

For crossâ€‘arch builds on x86â€‘64:

```bash
docker run --rm --privileged ghcr.io/linuxserver/qemu-static --reset
```

---

## ğŸ“œ  Changelog

See [CHANGELOG.md](./CHANGELOG.md) for a full release history.

---

## ğŸ“„  License

This project inherits the **GNU GPLâ€‘3.0** license from the upstream LinuxServer.io repository.
Additional modifications Â©Â 2025Â HorizonÂ AIÂ Notetaker.
