<!-- DO NOT EDIT THIS FILE MANUALLY -->

<!-- Please read https://github.com/the-horizon-dev/docker-fast-whisper/blob/main/.github/CONTRIBUTING.md -->

# the-horizon-dev/docker-fast-whisper

> **Fast, drop‑in Whisper API** – *Faster‑Whisper served over HTTP, inspired by the official OpenAI Whisper API.*

[Faster‑Whisper](https://github.com/SYSTRAN/faster-whisper) is a re‑implementation of OpenAI’s Whisper model built on top of **CTranslate2**, delivering dramatically faster inference on CPU and GPU alike. This container turns Faster‑Whisper into a self‑hosted service with an HTTP interface that mirrors the OpenAI Whisper API, so you can swap endpoints without rewriting your application code.

---

## 🙏 Credits & lineage

This image is a **fork** of the outstanding work by the **[LinuxServer.io](https://linuxserver.io) team**: [https://github.com/linuxserver/docker-faster-whisper](https://github.com/linuxserver/docker-faster-whisper).
We reused their build pipeline, base image and a good chunk of their documentation. Huge thanks to the LSIO maintainers for open‑sourcing the original container and CI setup!

---

## 🏗️  Supported architectures

Multi‑arch manifests are published so `docker pull ghcr.io/the-horizon-dev/fast-whisper:latest` will grab the correct image for your host:

| Architecture | Tag pattern         |
| ------------ | ------------------- |
| `x86‑64`     | `amd64-<version>`   |
| `arm64`      | `arm64v8-<version>` |

> **Note:** We currently do **not** ship an `armhf` build.

---

## 🔖  Version tags

| Tag      | Description                             |
| -------- | --------------------------------------- |
| `latest` | Stable CPU‑only build                   |
| `gpu`    | Same as `latest`, compiled with CUDA 12 |

---

## 🚀  Quick start

### Docker Compose (recommended)

```yaml
services:
  fast-whisper:
    image: ghcr.io/the-horizon-dev/fast-whisper:latest
    container_name: fast-whisper
    environment:
      - PUID=1000            # user id to run as
      - PGID=1000            # group id
      - TZ=Etc/UTC
      - WHISPER_MODEL=tiny-int8
      - WHISPER_BEAM=1       # optional
      - WHISPER_LANG=en      # optional
      - PORT=8000            # optional
    volumes:
      - /path/to/whisper/config:/config
    ports:
      - 8000:8000
    restart: unless-stopped
```

### Docker CLI

```bash
docker run -d \
  --name fast-whisper \
  -e PUID=1000 \
  -e PGID=1000 \
  -e TZ=Etc/UTC \
  -e WHISPER_MODEL=tiny-int8 \
  -e WHISPER_BEAM=1 \
  -e WHISPER_LANG=en \
  -e PORT=8000 \
  -v /path/to/whisper/config:/config \
  -p 8000:8000 \
  --restart unless-stopped \
  ghcr.io/the-horizon-dev/fast-whisper:latest
```

If you use the `gpu` tag, add the `--gpus` flag and make sure the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-container-toolkit) is installed on the host.

---

## 🔌  API usage

The container exposes endpoints that mimic the official OpenAI Whisper API:

* `POST /v1/audio/transcriptions` – transcribe a local file or remote URL
* `POST /v1/audio/translations` – translate non‑English speech to English

Send exactly the same JSON body you would send to `api.openai.com` – only the URL and your auth headers change.

See the [reference docs](https://github.com/SYSTRAN/faster-whisper#usage) for model‑specific query parameters.

---

## ⚙️  Parameters & environment variables

| Variable        | Description                                       | Default     |
| --------------- | ------------------------------------------------- | ----------- |
| `WHISPER_MODEL` | Model name (including `-int8` quantised variants) | `tiny-int8` |
| `WHISPER_BEAM`  | Number of decoding beams                          | `1`         |
| `WHISPER_LANG`  | ISO 639‑1 language code                           | `en`        |
| `PORT`          | Internal HTTP port                                | `8000`      |
| `PUID` / `PGID` | UID/GID the process runs as                       | `1000`      |
| `UMASK`         | Override default umask                            | `022`       |

All variables can also be provided through Docker *secrets* by prefixing the name with `FILE__`.

---

## 🔄  Updating the container

```bash
docker compose pull fast-whisper
docker compose up -d fast-whisper
```

Remember to prune dangling images afterwards:

```bash
docker image prune
```

---

## 🛠️  Building locally

```bash
git clone https://github.com/the-horizon-dev/docker-fast-whisper.git
cd docker-fast-whisper
docker build -t ghcr.io/the-horizon-dev/fast-whisper:latest .
```

For cross‑arch builds on x86‑64:

```bash
docker run --rm --privileged ghcr.io/linuxserver/qemu-static --reset
```

---

## 📜  Changelog

See [CHANGELOG.md](./CHANGELOG.md) for a full release history.

---

## 📄  License

This project inherits the **GNU GPL‑3.0** license from the upstream LinuxServer.io repository.
Additional modifications © 2025 Horizon AI Notetaker.
